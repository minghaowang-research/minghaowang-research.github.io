## CCT Competency 5: Assessing Student Learning

## Mentored Teaching Project: Pre-Course Attitudes and Group Performance

## Competency Description

Assessing Student Learning involves using summative and formative approaches to evaluate student learning outcomes effectively. This competency encompasses testing, grading, classroom assessment, course design, and portfolio assessment. For marketing education, this includes developing valid assessment instruments that measure both individual knowledge acquisition and collaborative learning outcomes.

## Project Overview

Research Question: "How do students' initial attitudes toward group work (self-rated collaboration skills and contribution patterns) predict their performance in collaborative marketing case study assignments compared to their individual exam performance?"

Context: MKT 327 Introduction to Marketing, Summer 2025
Sample: 63 students ( $94 \%$ finished pre-survey with 67 enrolled)
Duration: 6-week intensive course (July 1 - August 15, 2025)

## Assessment Instruments \& Data Collection

## 1. Pre-Course Assessment Survey

Purpose: Measure student attitudes and backgrounds before collaborative learning begins

## Instruments:

- Group work confidence: 5 -point Likert scale ( $1=$ very low to $5=$ very high)
- Expected contribution patterns: 4 -category scale (equal/more/much more/less than others)
- Marketing familiarity: 5-point scale (1=not familiar to 5=extremely familiar)
- Online comfort level: 5-point scale ( $1=$ very uncomfortable to 5=very comfortable)

Response Rate: 63/67 students (94\% completion)

## 2. Performance Assessment Data

## Group Work Assessment:

- Four collaborative case studies ( 100 points each, 400 points total)
- Companies: Nike/Mayo/Louis Vuitton, Chase/Apple/Uber, Redbull/Bestbuy/Airbnb, IKEA/Starbucks/Honest Tea
- Multiple format options: 200-300 words, 3-4 minute video, or infographic with explanation
- 14 groups of 5 students each, randomly assigned


## Individual Assessment:

- Three exams (200 points each, best 2 counted)
- Administered Weeks 3, 5, and 7
- Individual knowledge assessment of marketing concepts


## 3. Mid-Course Follow-up Survey

Purpose: Track attitude changes and actual collaboration experiences

- Collaboration experience ratings compared to initial expectations
- Marketing familiarity progression assessment


## Findings \& Data Analysis

## Sample Characteristics ( $\mathbf{n} \boldsymbol{=} \mathbf{6 3}$ )

## Pre-Course Attitudes:

- Group Confidence: $\mathrm{M}=4.30, \mathrm{SD}=0.80$ ( $85.7 \%$ rated medium-high to high confidence)
- Contribution Expectations: $47.6 \%$ expect equal contribution, $46.0 \%$ expect to contribute more, 6.3\% much more
- Marketing Familiarity: $\mathrm{M}=2.46, \mathrm{SD}=0.84$ (mostly slight to moderate familiarity)
- Online Comfort: $\mathrm{M}=3.86, \mathrm{SD}=1.22$ (full range of comfort levels)
![](https://cdn.mathpix.com/cropped/2025_10_17_2038725df7efa42a2af1g-2.jpg?height=622&width=1567&top_left_y=1563&top_left_x=271)

Expected Contribution Patterns
![](https://cdn.mathpix.com/cropped/2025_10_17_2038725df7efa42a2af1g-3.jpg?height=590&width=1575&top_left_y=281&top_left_x=279)

![](https://cdn.mathpix.com/cropped/2025_10_17_2038725df7efa42a2af1g-3.jpg?height=660&width=1622&top_left_y=940&top_left_x=268)

## Performance Outcomes

## Group Work Performance:

- Mean $=98.55 \%, \mathrm{SD}=1.15$ (range: $96.25 \%-100 \%$ )
- Extremely low variability (Coefficient of Variation $=1.17$ )
- Clear ceiling effect evident


## Individual Exam Performance:

- Mean = 91.86\%, SD = 3.05 (range: 81\%-98\%)
- Normal academic variability (Coefficient of Variation = 3.32)
- $7 \times$ higher variance compared to group work

Performance Distribution: Group Work vs Individual Exams
![](https://cdn.mathpix.com/cropped/2025_10_17_2038725df7efa42a2af1g-4.jpg?height=1009&width=1546&top_left_y=287&top_left_x=268)

Performance Gap: Group work scores systematically 6.7 percentage points higher than exam scores

## Primary Research Findings

## Correlation Analysis Results:

| Relationship | Correlation (r) | PValue | Effect Size | Significance |
| :--- | :--- | :--- | :--- | :--- |
| Group Confidence $\rightarrow$ Group Performance | -0.029 | 0.821 | Negligible | Not Significant |
| Contribution Pattern $\rightarrow$ Group Performance | 0.143 | 0.265 | Small | Not Significant |
| Group Confidence $\rightarrow$ Exam Performance | 0.184 | 0.148 | Small | Not Significant |
| Marketing Familiarity $\rightarrow$ Group Performance | -0.115 | 0.370 | Small | Not Significant |

Key Statistical Finding: No significant correlations found between pre-course attitudes and collaborative performance (all $\mathrm{p}>0.05$ ).
![](https://cdn.mathpix.com/cropped/2025_10_17_2038725df7efa42a2af1g-5.jpg?height=1012&width=1535&top_left_y=252&top_left_x=298)

## Assessment Design Insights

## Critical Discovery: Ceiling Effect in Group Assessment

The most significant finding was not about student attitudes but about assessment design itself. The group work assessment system created a ceiling effect ( $\mathrm{M}=98.55 \%, \mathrm{SD}=1.15 \%$ ) that masked individual differences and made performance prediction impossible.

## Evidence of Assessment Issues:

- Minimum group score: $96.25 \%$ (suggesting overly lenient grading)
- $7 \times$ less variance in group work compared to individual assessment
- Systematic 6.7 percentage point inflation in group scores


## Assessment Validity Analysis

Discriminant Validity: Group work assessment lacked sufficient discrimination between performance levels, suggesting the rubric was too generous or poorly calibrated.

Construct Validity: The assessment may have measured group coordination and time management more than marketing knowledge application.

Predictive Validity: Pre-course attitudes showed no predictive relationship with group performance, contrary to expectations based on collaborative learning literature.

## Pedagogical Implications \& Future Applications

## Teaching Philosophy Shift

FROM: "Right students in right groups" (selection-focused approach)
TO: "Right assessment design for all groups" (design-focused approach)
This research fundamentally changed my approach from student selection focus to assessment design focus, demonstrating that how we assess collaborative work matters more than how we form groups.

## Immediate Course Improvements for Summer 2026

1. Abandon Pre-Course Screening: Since attitudes don't predict performance, implement random or convenience-based group formation strategies.

## 2. Revise Group Assessment Rubric:

- Increase rigor and discrimination to create meaningful performance variance
- Develop scoring criteria that capture individual learning within group context
- Align assessment difficulty with individual exam standards


## 3. Add Individual Accountability Measures:

- Implement peer evaluation systems
- Include individual components within group projects
- Track individual contributions within collaborative work (Thru Google)


## 4. Enhanced Assessment Design:

- Focus professional development on assessment methodology rather than group formation strategies
- Develop multiple assessment points within each case study


## Methodological Insights for Marketing Education

## Assessment Design Principles:

- Collaborative assessments must maintain appropriate difficulty levels to avoid ceiling effects
- Rubric design significantly impacts the validity of group work evaluation
- Individual accountability within group work is essential for meaningful assessment


## Student Learning Outcomes:

- Students successfully developed collaborative problem-solving skills despite assessment limitations
- Marketing concept application occurred effectively within group context
- Peer connections and engagement objectives were achieved


## Artifacts \& Evidence

## Research Documentation

- 6-Step Mentored Teaching Project Worksheet: Complete systematic analysis following CCT framework
- Project Approval Document: Formal mentor approval and assessment plan
- Statistical Analysis Code: Comprehensive R analysis


## Assessment Instruments

- Pre-course attitude survey ( $94 \%$ response rate)
- Mid-course follow-up survey (tracking attitude changes)
- Group case study rubrics and performance data
- Individual exam assessment data


## Data Analysis Results

- Complete statistical analysis with correlation matrices, descriptive statistics, and effect size calculations
- Visual data representations (demographic distributions, performance comparisons, correlation analyses)


## Reflection on Assessment Practice

## What I Learned

Primary Discovery: The most valuable learning was about assessment design rather than group formation. The data revealed that my assessment system was the limiting factor in understanding student performance, not student characteristics.

Assessment Expertise Development: This project developed my ability to:

- Design and implement systematic data collection for teaching assessment
- Analyze assessment validity and reliability issues
- Identify ceiling effects and discrimination problems in rubric design
- Connect statistical findings to pedagogical practice


## What Surprised Me

1. Complete Lack of Predictive Validity: Even small effect sizes were not consistently observed between attitudes and performance.
2. Magnitude of Assessment Issues: The $98.55 \%$ average with $96.25 \%$ minimum revealed more about my grading practices than student capabilities.
3. Stronger Individual Patterns: Pre-course attitudes showed slightly stronger (though still non-significant) relationships with individual work, opposite of expectations.

## Impact on Future Teaching Practice

Evidence-Based Teaching Strategy: This research provides concrete evidence for future course design decisions, moving from intuition-based to data-driven approaches.

Assessment-Focused Professional Development: The findings direct my continued learning toward rubric design, validity assessment, and collaborative learning evaluation methods.

## Conclusion

This Mentored Teaching Project demonstrates comprehensive competency in assessing student learning through systematic research design, valid data collection, statistical analysis, and pedagogical application. The project's most significant contribution is revealing how assessment design impacts our ability to understand and support student learning in collaborative contexts.

The research provides actionable insights for improving marketing education assessment practices while contributing to broader understanding of collaborative learning evaluation.

