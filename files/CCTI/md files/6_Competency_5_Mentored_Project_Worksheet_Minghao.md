Teaching and Learning Goal: What skill or ability do you want students to acquire? What behavior do you want to change? What knowledge do you want to test? What assumptions (either students' or the instructor's) do you want to test? Focus on only one such goal

I want to examine how students' perspectives on group work influence their performance in collaborative case study assignments. This assumption underlies many group formation strategies in business education, where instructors believe that matching students based on initial attitudes will lead to better collaborative outcomes.

Focus: Testing whether pre-course screening for group formation is pedagogically valuable or if other factors (assessment design, grading practices) are more influential in collaborative learning success.

Teaching Question: Adapt the teaching and learning goal to a specific course. Make this question narrow and focused so that it can be measured.

Narrow, focused, and measurable question for MKT 327:
"How do students' initial attitudes toward group work (self-rated collaboration skills and expected contribution patterns) predict their performance in collaborative marketing case study assignments compared to their individual exam performance?"

Specific Hypotheses:

- H1: Higher pre-course group work confidence will correlate positively with group case study performance
- H2: Students expecting to contribute "more than others" will perform better in group assignments
- H3: Pre-course attitudes will show stronger relationships with group work than individual assessment performance

Assessment Technique: What instrument are you going to use to collect information? Is it simple enough that you know how to analyze the results? Will the information it provides answer the teaching question?

## Data Collection Instruments:

1. Pre-Course Survey (Week 1)

- Group work confidence: 5-point Likert scale (1=very low to 5=very high)
- Expected contribution patterns: 5-point scale coded into 4 categories (equal/more/much more/less than others)
- Marketing familiarity: 5-point Likert scale (1=not familiar to 5=extremely familiar)
- Online comfort level: 5-point Likert scale (1=very uncomfortable to 5=very comfortable)

2. Performance Metrics

- Group case study scores: 4 assignments $\times 100$ points each
- Individual exam scores: 3 exams $\times 200$ points each (best 2 counted)
- Final course performance metrics

3. Mid-Course Survey (Week 3)

- Actual collaboration experience ratings
- Marketing familiarity progression

Analysis Plan: Pearson correlations between 5-point Likert attitude scales and percentage performance outcomes, descriptive statistics, effect size calculations using Cohen's conventions, variance analysis to detect ceiling effects.

Classroom Practice: What assignment or activity are you going to use in the class to try to test the question? When are you going to do it? Who will conduct it? Will it be graded? Will it be anonymous or will students sign their names? How long will it take? How will students know what to do with it? Who will explain it? How will the relationship between this assignment and activity and the course be explained?

Implementation Details:
When: Summer 2025 (July 1 - August 15, 6-week intensive course)
Who: 70 student enrolled, 67 students stayed, 63 completed the pre-survey ( $94 \%$ )

## Structure:

- 14 groups of 5 students each
- Random group assignment (no pre-course screening used)
- Groups remained constant throughout course


## Graded Assignments:

- Week 1: Group Case Study 1 (Nike, Mayo, Louis Vuitton)
- Week 2: Group Case Study 2 (Chase, Apple, Uber)
- Week 4: Group Case Study 3 (Redbull, Bestbuy, Airbnb)
- Week 6: Group Case Study 4 (IKEA, Starbucks, Honest Tea)


## Individual Assessments:

- Week 3: Exam 1 (200 points)
- Week 5: Exam 2 (200 points)
- Week 7: Exam 3 (200 points)

Course Integration: Group case studies comprised $40 \%$ of final grade, representing a significant pedagogical shift from previous individual-based assessments to collaborative learning format.

Summary of Results: What does the information you collected through the assessment instrument tell you about your teaching question?
Sample Characteristics ( $\mathbf{n} \boldsymbol{=} \mathbf{6 3}$ )

## Pre-Course Attitudes:

- Group Confidence: $\mathrm{M}=4.30, \mathrm{SD}=0.80$ (5-point scale, range: 2-5)
- $85.7 \%$ rated confidence as medium-high to high ( $4-5$ on scale)
- Contribution Expectations: $47.6 \%$ expect equal contribution, $46.0 \%$ expect to contribute more, $6.3 \%$ much more
- Marketing Familiarity: $\mathrm{M}=2.46, \mathrm{SD}=0.84$ (5-point scale, range: $1-4$, mostly slight to moderate)

See Classroom Assessment Techniques p. 59 for a helpful "Checklist for Avoiding Problems with Classroom Assessment Projects"

- Online Comfort: $\mathrm{M}=3.86, \mathrm{SD}=1.22$ (5-point scale, full range utilized)


## Performance Outcomes

## Group Work Performance:

- Mean $=98.55 \%, \mathrm{SD}=1.15$ (range: $96.25 \%-100 \%$ )
- Coefficient of Variation = 1.17 (extremely low variability)
- Clear ceiling effect evident


## Individual Exam Performance:

- Mean $=91.86 \%, \mathrm{SD}=3.05$ (range: $81 \%-98 \%$ )
- Coefficient of Variation $=3.32 \%$ (normal academic variability)
- $7 \times$ higher variance compared to group work


## Primary Research Findings

## Correlation Analysis Results:

| Relationship | r | p-value | Effect Size | Significance |
| :--- | :--- | :--- | :--- | :--- |
| Group Confidence $\rightarrow$ Group Performance | -0.029 | 0.821 | Negligible | Not Significant |
| Contribution Pattern $\rightarrow$ Group Performance | 0.143 | 0.265 | Small | Not Significant |
| Group Confidence $\rightarrow$ Exam Performance | 0.184 | 0.148 | Small | Not Significant |
| Marketing Familiarity $\rightarrow$ Group Performance | -0.115 | 0.370 | Small | Not Significant |

Key Statistical Finding: No significant correlations were found between any pre-course attitudes and collaborative performance (all $\mathrm{p}>0.05$ ).

Unexpected Discovery: Group work scores were systematically 6.7 percentage points higher than individual exam scores, with dramatically reduced variance, indicating a strong ceiling effect in group assessment.

Conclusion: What have you learned? What surprised you? What would you do differently? What implications does this have for your future classroom practice?

## What I Learned

Primary Discovery: Pre-course attitudes toward group work do not predict collaborative performance in marketing case studies. The hypothesis that initial confidence and contribution expectations would correlate with group success was not supported.

Most Significant Finding: The group work assessment system created a ceiling effect that masked individual differences and made performance prediction impossible. The extremely low variance ( $\mathrm{SD}=1.15 \%$ ) compared to individual exams ( $\mathrm{SD}=3.05 \%$ ) suggests the grading rubric was too lenient.

## What Surprised Me

1. Complete lack of predictive validity - Even small effect sizes were not consistently observed
2. Magnitude of the ceiling effect - $98.55 \%$ average with $96.25 \%$ minimum suggests assessment lacks discrimination
3. Stronger attitude-performance patterns appeared with individual work rather than group work, opposite of expectations

## What I Would Do Differently

## Immediate Changes for Summer 2026:

1. Abandon Pre-Course Screening: Since attitudes don't predict performance, use random or convenience-based group formation
2. Revise Group Assessment Rubric: Increase rigor and discrimination to create meaningful performance variance
3. Add Individual Accountability: Implement peer evaluation systems and individual components within group projects
4. Enhanced Assessment Design: Focus on assessment methodology rather than group formation strategies

## Methodological Improvements:

- Collect qualitative data on group dynamics
- Include peer evaluation metrics
- Track individual contributions within groups
- Implement multiple assessment points within each case study


## Implications for Future Classroom Practice

Evidence-Based Teaching Strategy: This research fundamentally changed my approach from student selection focus to assessment design focus. The data clearly shows that how we assess collaborative work matters more than how we form groups.

## Pedagogical Philosophy Shift:

- FROM: "Right students in right groups"
- TO: "Right assessment design for all groups"


## Practical Applications:

1. Maintain collaborative learning benefits while improving assessment rigor
2. Develop rubrics that capture individual contribution and learning
3. Focus professional development on assessment design rather than group dynamics
4. Share findings with marketing education community to improve collaborative learning practices

Broader Impact: This project demonstrates the value of systematic teaching inquiry. What began as a practical question about group formation revealed fundamental issues with assessment design that likely affect student learning outcomes across multiple offerings of the course.

## Future Research Questions:

- How do different rubric designs affect the validity of group work assessment?
- What individual accountability measures maintain collaborative benefits while improving discrimination?
- How do these findings apply to other business education contexts?

